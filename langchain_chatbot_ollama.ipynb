{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랭체인 챗봇 만들기\n",
    "- LangChain과 ChatOllama 모델을 사용하여 기본적인 챗봇 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T14:55:02.109580Z",
     "start_time": "2025-11-24T14:55:01.105173Z"
    }
   },
   "outputs": [],
   "source": [
    "###%pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:02:37.594671Z",
     "start_time": "2025-11-24T15:02:37.577842Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"deepseek-r1:1.5b\") # 자기 설치 버전에 맞게 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫 번째 대화\n",
    "- 딥시크 R1 모델은 <think> </think> 태그 사이에 자신이 해야 할 일을 생각한 후 답변함\n",
    "- </think> 이후 메시지만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:04:53.458705Z",
     "start_time": "2025-11-24T15:04:53.456620Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"너는 사용자를 도와주는 상담사야. 한국어만 사용해서 아시아의 나라들에 장단점을 대답해\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:05:51.825382Z",
     "start_time": "2025-11-24T15:04:56.090786Z"
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"사용자: \")\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "\n",
    "    messages.append(\n",
    "        HumanMessage(user_input)\n",
    "    )\n",
    "\n",
    "    response = model.stream(messages) # invoke 대신 stream으로 출력\n",
    "\n",
    "    # stream 형식으로 출력\n",
    "    ai_message = None\n",
    "    for chunk in response:\n",
    "        print(chunk.content, end=\"\")\n",
    "        if ai_message is None:\n",
    "            ai_message = chunk\n",
    "        else:\n",
    "            ai_message += chunk\n",
    "    print('')\n",
    "\n",
    "    message_only = ai_message.content.strip()\n",
    "    #message_only = ai_message.content.split(\"</think>\")[1].strip()\n",
    "    messages.append(AIMessage(message_only))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
